{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Sep  6 01:35:32 2019\n",
    "\n",
    "@author: Phong\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PATR 0: Import important functions ##################################\n",
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import functools\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "#kurtosis_pearson = functools.partial(kurtosis, fisher=False)\n",
    "#skew_p = functools.partial(skew)\n",
    "#std_p = functools.partial(std)\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Memory management\n",
    "import gc\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    # data = housing\n",
    "    np.random.seed(42)\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data)*test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "def remove_missing_columns(train, test, threshold = 99):\n",
    "    # Calculate missing stats for train and test (remember to calculate a percent!)\n",
    "    train_miss = pd.DataFrame(train.isnull().sum())\n",
    "    train_miss['percent'] = 100 * train_miss[0] / len(train)\n",
    "    \n",
    "    test_miss = pd.DataFrame(test.isnull().sum())\n",
    "    test_miss['percent'] = 100 * test_miss[0] / len(test)\n",
    "    \n",
    "    # list of missing columns for train and test\n",
    "    missing_train_columns = list(train_miss.index[train_miss['percent'] > threshold])\n",
    "    missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n",
    "    \n",
    "    # Combine the two lists together\n",
    "    missing_columns = list(set(missing_train_columns + missing_test_columns))\n",
    "    \n",
    "    # Print information\n",
    "    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n",
    "    \n",
    "    # Drop the missing columns and return\n",
    "    train = train.drop(columns = missing_columns)\n",
    "    test = test.drop(columns = missing_columns)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PATR 1: Features Engineering ##################################\n",
    "\n",
    "\n",
    "####. 1_Function to Aggregate Numeric Data\n",
    "def agg_numeric(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Groups and aggregates the numeric values in a child dataframe\n",
    "    by the parent variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe): \n",
    "            the child dataframe to calculate the statistics on\n",
    "        parent_var (string): \n",
    "            the parent variable used for grouping and aggregating\n",
    "        df_name (string): \n",
    "            the variable used to rename the columns\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated by the `parent_var` for \n",
    "            all numeric columns. Each observation of the parent variable will have \n",
    "            one row in the dataframe with the parent variable as the index. \n",
    "            The columns are also renamed using the `df_name`. Columns with all duplicate\n",
    "            values are removed. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in df:\n",
    "        if col != parent_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns = col)\n",
    "            \n",
    "    # Only want the numeric variables\n",
    "    parent_ids = df[parent_var].copy()\n",
    "    numeric_df = df.select_dtypes('number').copy()\n",
    "    numeric_df[parent_var] = parent_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum','var','std', 'skew'])#]) # With 'std': problem with NA\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = []\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        if var != parent_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    agg.columns = columns\n",
    "    \n",
    "    # Remove the columns with all redundant values\n",
    "    _, idx = np.unique(agg, axis = 1, return_index=True)\n",
    "    agg = agg.iloc[:, idx]\n",
    "    \n",
    "    return agg\n",
    "\n",
    "\n",
    "#### 2. Function to calculate categorical counts\n",
    "# normed count, which is the count for a category divided by the total counts for all categories in a categorical variable.\n",
    "# counts: the occurrences  of each category in a categorical variable \n",
    "\n",
    "def agg_categorical(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Aggregates the categorical features in a child dataframe\n",
    "    for each observation of the parent variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe \n",
    "        The dataframe to calculate the value counts for.\n",
    "        \n",
    "    parent_var : string\n",
    "        The variable by which to group and aggregate the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "        \n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with aggregated statistics for each observation of the parent_var\n",
    "        The columns are also renamed and columns with duplicate values are removed.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the categorical columns\n",
    "    categorical = pd.get_dummies(df.select_dtypes('category'))\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[parent_var] = df[parent_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(parent_var).agg(['sum', 'count','mean'])\n",
    "    \n",
    "    column_names = []\n",
    "    \n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['sum', 'count', 'mean']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    \n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    # Remove duplicate columns by values\n",
    "    _, idx = np.unique(categorical, axis = 1, return_index = True)\n",
    "    categorical = categorical.iloc[:, idx]\n",
    "    \n",
    "    return categorical\n",
    "\n",
    "    # add1 = agg_categorical(bureau1 , parent_var = 'SK_ID_CURR', df_name = 'app')\n",
    "    \n",
    "# Function to Aggregate Stats at the Client Level\n",
    "\n",
    "def aggregate_client(df, group_vars, df_names):\n",
    "    \"\"\"Aggregate a dataframe with data at the loan level \n",
    "    at the client level\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): data at the loan level\n",
    "        group_vars (list of two strings): grouping variables for the loan \n",
    "        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n",
    "        names (list of two strings): names to call the resulting columns\n",
    "        (example ['cash', 'client'])\n",
    "        \n",
    "    Returns:\n",
    "        df_client (dataframe): aggregated numeric stats at the client level. \n",
    "        Each client will have a single row with all the numeric data aggregated\n",
    "    \"\"\"\n",
    "    # Aggregate the numeric columns\n",
    "    df_agg = agg_numeric(df, parent_var = group_vars[0], df_name=df_names[0])\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    if any(df.dtypes == 'category'):\n",
    "        df_counts = agg_categorical(df, parent_var = group_vars[0], df_name = df_names[0])\n",
    "        \n",
    "        # Merge 2 dfs:\n",
    "        df_by_loan1 = df_counts.merge(df_agg, on = group_vars[0], how = 'outer')\n",
    "        gc.enable()\n",
    "        del df_agg, df_counts\n",
    "        gc.collect()\n",
    "        \n",
    "        # # Merge to get the client id in dataframe\n",
    "        \n",
    "        df_by_loan1 = df_by_loan1.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n",
    "        \n",
    "        # remove the loan id\n",
    "        \n",
    "        df_by_loan1 = df_by_loan1.drop(columns= [group_vars[0]])\n",
    "        # Aggregate numeric stats by column\n",
    "        df_by_client = agg_numeric(df_by_loan1, parent_var = group_vars[1], df_name = df_names[1])\n",
    "    # No categorical variables\n",
    "    else:        \n",
    "        df_by_loan1 = df_agg.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how ='left')\n",
    "        gc.enable()\n",
    "        del df_agg\n",
    "        gc.collect()\n",
    "        \n",
    "        # Remove the loan id\n",
    "        df_by_loan1 = df_by_loan1.drop(columns = [group_vars[0]])\n",
    "        # Aggregate numeric stats by column\n",
    "        df_by_client = agg_numeric(df_by_loan1, parent_var = group_vars[1], df_name = df_names[1])\n",
    "        \n",
    "    # Memory management\n",
    "    gc.enable()\n",
    "    del df, df_by_loan1\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_by_client\n",
    "\n",
    "\n",
    "\n",
    "# Function to Convert Data Types\n",
    "# This will help reduce memory usage by using more efficient types for the variables: or example category is often a better type than object\n",
    "import sys\n",
    "\n",
    "def return_size(df):\n",
    "    \"\"\"Return size of dataframe in gigabytes\"\"\"\n",
    "    return round(sys.getsizeof(df) / 1e9, 2)\n",
    "\n",
    "def convert_types(df, print_info = False):\n",
    "    \n",
    "    original_memory = df.memory_usage().sum()\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    new_memory = df.memory_usage().sum()\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################################################################################################################\n",
    "######################## START FEATURES ENGINEERING HERE #################################################################################################################\n",
    "#############################################################################################################################################################################\n",
    "\n",
    "\n",
    "##################################### 0. Application train Data set #################################################################################################\n",
    "    \n",
    "\n",
    "app_train = pd.read_csv('application_train.csv')\n",
    "############### Create Domain Knowledge features\n",
    "# CREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income\n",
    "# ANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income\n",
    "# CREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due\n",
    "# DAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age\n",
    "\n",
    "app_train['CREDIT_INCOME_PERCENT'] = app_train['AMT_CREDIT']/app_train['AMT_INCOME_TOTAL']\n",
    "app_train['ANNUITY_INCOME_PERCENT'] = app_train['AMT_ANNUITY']/app_train['AMT_INCOME_TOTAL']\n",
    "app_train['CREDIT_TERM'] = app_train['AMT_ANNUITY']/app_train['AMT_CREDIT']\n",
    "app_train['DAYS_EMPLOYED_PERCENT'] = app_train['DAYS_EMPLOYED']/app_train['DAYS_BIRTH']\n",
    "app_train['INCOME_PER_PERSON'] = app_train['AMT_INCOME_TOTAL'] / app_train['CNT_FAM_MEMBERS']\n",
    "app_train['PAYMENT_RATE'] = app_train['AMT_ANNUITY'] / app_train['AMT_CREDIT']\n",
    "\n",
    "train_set, test_set = split_train_test(train_set, 0.2)\n",
    "\n",
    "train_set.to_csv('train_set.csv', index = False)\n",
    "test_set.to_csv('test_set.csv', index = False)\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################################################################################################################\n",
    "######################## STAGE 1: Features from application_train, bureau and bureau_balance ################################################################################\n",
    "########### 1_ Bureau Data Set ###############################################################################################################################################\n",
    "train = pd.read_csv('train_set.csv')\n",
    "test = pd.read_csv('test_set.csv')\n",
    "bureau = pd.read_csv('bureau.csv')#.head(50000)\n",
    "bureau = convert_types(bureau, print_info = True)\n",
    "bureau.info()\n",
    "\n",
    "previous_loan_counts = bureau.groupby('SK_ID_CURR', as_index = False)['SK_ID_BUREAU'].count().rename(columns ={'SK_ID_BUREAU': 'previous_loan_counts'})\n",
    "previous_loan_counts.head()\n",
    "train = train.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n",
    "train['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)\n",
    "train.info()\n",
    "\n",
    "test = test.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n",
    "test['previous_loan_counts'] = test['previous_loan_counts'].fillna(0)\n",
    "test.info()\n",
    "\n",
    "bureau['CREDIT_DAY_OVERDUE_TIME_DAYS_CREDIT'] = bureau['CREDIT_DAY_OVERDUE'] * bureau['DAYS_CREDIT']\n",
    "bureau_by_client = aggregate_client(bureau, group_vars =['SK_ID_BUREAU', 'SK_ID_CURR'], df_names = ['bureau', 'client'])\n",
    "\n",
    "list(bureau_by_client.columns)\n",
    "\n",
    "train=train.merge(bureau_by_client, on = 'SK_ID_CURR', how = 'left' )\n",
    "\n",
    "test = test.merge(bureau_by_client, on = 'SK_ID_CURR', how = 'left' )\n",
    " \n",
    "gc.enable()\n",
    "del bureau , bureau_by_client \n",
    "gc.collect()\n",
    "train, test = remove_missing_columns(train, test)\n",
    "train.info()\n",
    "\n",
    "########### 2_ Bureau_balance Data Set ######################################################################################################################################\n",
    "bureau_balance = pd.read_csv('bureau_balance.csv')\n",
    "bureau_balance.head()\n",
    "bureau = pd.read_csv('bureau.csv')[['SK_ID_BUREAU', 'SK_ID_CURR']]\n",
    "bureau_balance = bureau_balance.merge(bureau, on ='SK_ID_BUREAU', how = 'left')\n",
    "\n",
    "bureau_balance = convert_types(bureau_balance, print_info = True)\n",
    "bureau_balance.info()\n",
    "\n",
    "bureau_balance_by_client = aggregate_client(bureau_balance, group_vars =['SK_ID_BUREAU', 'SK_ID_CURR'], df_names = ['bureau_balance', 'client'])\n",
    "\n",
    "bureau_balance_by_client.head()\n",
    "\n",
    "train=train.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "test = test.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "gc.enable()\n",
    "del bureau_balance_by_client, bureau_balance, bureau\n",
    "gc.collect()\n",
    "train, test = remove_missing_columns(train, test)\n",
    "train.info()\n",
    "\n",
    "train.to_csv('train_after_stage1.csv', index = False)\n",
    "test.to_csv('test_after_stage1.csv', index = False)\n",
    "\n",
    "train.info()\n",
    "'TARGET' in list(train.columns)\n",
    "'TARGET' in list(test.columns)\n",
    "set(list(train.columns)) - set(list(test.columns))\n",
    "test.info()\n",
    "\n",
    "##################################### STAGE 2 ###################################################################################################################\n",
    "################ All data except for ata from Installment Payments ##############################################################################################\n",
    "########### 3_ previous_application Data Set #############################################################################################################################\n",
    "\n",
    "previous=pd.read_csv('previous_application.csv')\n",
    "previous = convert_types(previous, print_info=True)\n",
    "previous.head()\n",
    "\n",
    "previous_agg = agg_numeric(previous, 'SK_ID_CURR', 'previous')\n",
    "previous_agg.shape # 37 columns -> 70 columns\n",
    "\n",
    "\n",
    "previous_counts = agg_categorical(previous, 'SK_ID_CURR', 'previous')\n",
    "previous_counts.shape # 37 columns -> 285 columns\n",
    "list(previous_counts.columns)\n",
    "\n",
    "# train = pd.read_csv('train_after_stage1.csv')\n",
    "train = convert_types(train)\n",
    "\n",
    "# test =pd.read_csv('test_after_stage1.csv')\n",
    "\n",
    "test.info()\n",
    "test = convert_types(test)\n",
    "\n",
    "# Merge new features into train and test\n",
    "train = train.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\n",
    "train = train.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "test = test.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\n",
    "test = test.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Remove variables to free memory\n",
    "gc.enable()\n",
    "del previous, previous_agg, previous_counts\n",
    "gc.collect()\n",
    "\n",
    "train, test = remove_missing_columns(train, test)\n",
    "\n",
    "########### 4_ Monthly Cash Data Set####################################################################################################################################\n",
    "cash = pd.read_csv('POS_CASH_balance.csv')\n",
    "cash = convert_types(cash, print_info = True)\n",
    "cash.head()\n",
    "cash.info()\n",
    "\n",
    "cash_by_client = aggregate_client(cash, group_vars =['SK_ID_PREV', 'SK_ID_CURR'], df_names =['cash', 'client'])\n",
    "cash_by_client.info()\n",
    "cash_by_client.head()\n",
    "\n",
    "print('Cash by client Shape: ', cash_by_client.shape)\n",
    "train = train.merge(cash_by_client, on ='SK_ID_CURR', how ='left')\n",
    "test = test.merge(cash_by_client, on = 'SK_ID_CURR', how ='left')\n",
    "\n",
    "gc.enable()\n",
    "del cash, cash_by_client\n",
    "gc.collect()\n",
    "\n",
    "train, test = remove_missing_columns(train, test)\n",
    "\n",
    "########### 5_ Monthly Credit Data Set ##################################################################################################################################\n",
    "\n",
    "credit = pd.read_csv('credit_card_balance.csv')\n",
    "credit = convert_types(credit, print_info = True)\n",
    "credit.info()\n",
    "credit.head()\n",
    "\n",
    "credit_by_client = aggregate_client(credit, group_vars=['SK_ID_PREV', 'SK_ID_CURR'], df_names=['credit','client'])\n",
    "credit_by_client.head()\n",
    "\n",
    "train = train.merge(credit_by_client, on = 'SK_ID_CURR', how ='left')\n",
    "test = test.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "gc.enable()\n",
    "del credit, credit_by_client\n",
    "gc.collect()\n",
    "train, test = remove_missing_columns(train, test)\n",
    "##################################################################################################################################################################\n",
    "########################################## STAGE 3: include data from Installment Payments ######################################################################\n",
    "########### 6_Installment Payments Data Set #############################################################################################################################\n",
    "\n",
    "installments =pd.read_csv('installments_payments.csv')\n",
    "installments = convert_types(installments, print_info = True)\n",
    "installments.info()\n",
    "installments.head()\n",
    "\n",
    "\n",
    "installments_by_client = aggregate_client(installments, group_vars =['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\n",
    "\n",
    "installments_by_client.head()\n",
    "\n",
    "train=train.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left' )\n",
    "\n",
    "test = test.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left' )\n",
    "\n",
    "gc.enable()\n",
    "del installments, installments_by_client\n",
    "gc.collect()\n",
    "train, test = remove_missing_columns(train, test)\n",
    "train.info()\n",
    "test.info()\n",
    "\n",
    "test['TARGET'] = test_labels\n",
    "\n",
    "print(f'Final training size: {return_size(train)}')\n",
    "print(f'Final testing size: {return_size(test)}')\n",
    "\n",
    "train.to_csv('train_after_stage3.csv', index = False)\n",
    "test.to_csv('test_after_stage3.csv', index = False)\n",
    "set(list(train.columns)) - set(list(test.columns))\n",
    "\n",
    "\n",
    "################################ STAGE 4 ########################################################################################################################\n",
    "############################## Create more polinomial features for data after stage 3 ##################################################################################################\n",
    "################################# ###############################################################################################################################\n",
    "train = pd.read_csv('train_after_stage3.csv')\n",
    "app_train = train # convert_types(train)\n",
    "\n",
    "test =pd.read_csv('test_after_stage3.csv')\n",
    "\n",
    "app_test = test#convert_types(test) \n",
    "\n",
    "\n",
    "######################## Create the Polynomial Features from most important 8 feactors:\n",
    "\n",
    "\n",
    "\n",
    "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH',\n",
    "                           'AMT_CREDIT','AMT_ANNUITY', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'DAYS_EMPLOYED']]\n",
    "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH',\n",
    "                           'AMT_CREDIT','AMT_ANNUITY', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'DAYS_EMPLOYED']]\n",
    "\n",
    "\n",
    "# Imputer for handling the missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy = 'median')\n",
    "poly_target = app_train['TARGET']\n",
    "\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "# Create the polynomial object with specified degree: add 212 features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "poly_transformer.fit(poly_features)\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape: ', poly_features.shape, poly_features_test.shape)\n",
    "column = poly_transformer.get_feature_names(input_features =['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH',\n",
    "                           'AMT_CREDIT','AMT_ANNUITY', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'DAYS_EMPLOYED'])\n",
    "\n",
    "# Create a dataframe of the features\n",
    "poly_features = pd.DataFrame(poly_features, columns = column)\n",
    "poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n",
    "poly_features_test = pd.DataFrame(poly_features_test, columns = column)\n",
    "poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n",
    "poly_features_test.shape\n",
    "poly_features.shape\n",
    "# Merger to app_train and app_test:\n",
    "app_train = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n",
    "app_train.shape\n",
    "app_test = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how ='left')\n",
    "app_test.shape\n",
    "\n",
    "\n",
    "\n",
    "app_train.to_csv('train_after_stage4.csv', index = False)\n",
    "app_test.to_csv('test_after_stage4.csv', index = False)\n",
    "\n",
    "#####################################################################################################################################################################\n",
    "############### ROUND 2: Create More Knowledge Features and Time Features ##########################################################################################\n",
    "###################################################################################################################################################################################################\n",
    "\n",
    "############# 0.Application_train Data Set #################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "#df = pd.read_csv('application_train.csv')\n",
    "df.head()\n",
    "df = train \n",
    "df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "# Time features;\n",
    "df['train_NEW_EMPLOY_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "df['train_DAYS_EMPLOYED - DAYS_BIRTH'] = df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']\n",
    "df['train_NEW_CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "df['train_NEW_CAR_TO_EMPLOY_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "df['train_NEW_PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "df['train_NEW_PHONE_TO_EMPLOY_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
    "df['train_EXT_SOURCE_1 / DAYS_BIRTH'] = df['EXT_SOURCE_1'] / df['DAYS_BIRTH']\n",
    "df['train_EXT_SOURCE_2 / DAYS_BIRTH'] = df['EXT_SOURCE_2'] / df['DAYS_BIRTH']\n",
    "df['train_EXT_SOURCE_3 / DAYS_BIRTH'] = df['EXT_SOURCE_3'] / df['DAYS_BIRTH']\n",
    "# Knowledge features:\n",
    "df['train_NEW_CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "df['train_NEW_CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "df['train_NEW_INC_PER_CHLD'] = df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])\n",
    "df['train_NEW_ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / (1 + df['AMT_INCOME_TOTAL'])\n",
    "df['train_NEW_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "df['train_NEW_EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "df['train_NEW_SCORES_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "df['train_NEW_SCORES_STD'] = df['NEW_SCORES_STD'].fillna(df['NEW_SCORES_STD'].mean())\n",
    "df['train_NEW_CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "dropcolum=['FLAG_DOCUMENT_2','FLAG_DOCUMENT_4',\n",
    "    'FLAG_DOCUMENT_5','FLAG_DOCUMENT_6','FLAG_DOCUMENT_7',\n",
    "    'FLAG_DOCUMENT_8','FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', \n",
    "    'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13',\n",
    "    'FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16',\n",
    "    'FLAG_DOCUMENT_17','FLAG_DOCUMENT_18','FLAG_DOCUMENT_19',\n",
    "    'FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']\n",
    "df= df.drop(dropcolum,axis=1)\n",
    "\n",
    "train = df\n",
    "\n",
    "######## 1. bureau #################################################################################################################################################\n",
    "# bureau: information about client's previous loans with other financial institutions reported to Home Credit. Each previous loan has its own row.\n",
    "# bureau_balance: monthly information about the previous loans.\n",
    "# Each month has its own row.\n",
    "# https://www.kaggle.com/shanth84/home-credit-bureau-data-feature-engineering\n",
    "\n",
    "# UNDERSTANDING OF VARIABLES¶\n",
    "# CREDIT_ACTIVE - Current status of a Loan - Closed/ Active (2 values)\n",
    "\n",
    "# CREDIT_CURRENCY - Currency in which the transaction was executed - Currency1, Currency2, Currency3, Currency4 ( 4 values)\n",
    "\n",
    "# CREDIT_DAY_OVERDUE - Number of overdue days\n",
    "\n",
    "# CREDIT_TYPE - Consumer Credit, Credit card, Mortgage, Car loan, Microloan, Loan for working capital replemishment, Loan for Business development, Real estate loan, Unkown type of laon, Another type of loan. Cash loan, Loan for the purchase of equipment, Mobile operator loan, Interbank credit, Loan for purchase of shares ( 15 values )\n",
    "\n",
    "# DAYS_CREDIT - Number of days ELAPSED since customer applied for CB credit with respect to current application Interpretation - Are these loans evenly spaced time intervals? Are they concentrated within a same time frame?\n",
    "\n",
    "# DAYS_CREDIT_ENDDATE - Number of days the customer CREDIT is valid at the time of application CREDIT_DAY_OVERDUE - Number of days the customer CREDIT is past the end date at the time of application\n",
    "\n",
    "# AMT_CREDIT_SUM - Total available credit for a customer AMT_CREDIT_SUM_DEBT - Total amount yet to be repayed\n",
    "# AMT_CREDIT_SUM_LIMIT - Current Credit that has been utilized\n",
    "# AMT_CREDIT_SUM_OVERDUE - Current credit payment that is overdue\n",
    "# CNT_CREDIT_PROLONG - How many times was the Credit date prolonged\n",
    "bureau = pd.read_csv('bureau.csv')\n",
    "bureau = convert_types(bureau, print_info = True)\n",
    "bureau.info()\n",
    "# FEATURE 1 - NUMBER OF PAST LOANS PER CUSTOMER\n",
    "previous_loan_counts = bureau.groupby('SK_ID_CURR', as_index = False)['SK_ID_BUREAU'].count().rename(columns ={'SK_ID_BUREAU': 'previous_loan_counts'})\n",
    "previous_loan_counts.head()\n",
    "train = train.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n",
    "train['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)\n",
    "train.info()\n",
    "\n",
    "# bureau_time_features =bureau[['SK_ID_CURR','DAYS_CREDIT_UPDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_ENDDATE',\n",
    "#                       'CREDIT_DAY_OVERDUE', 'DAYS_CREDIT']]\n",
    "# New time features\n",
    "bureau['bureau_CREDIT_DAY_OVERDUE_TIME_DAYS_CREDIT'] = bureau['CREDIT_DAY_OVERDUE'] * bureau['DAYS_CREDIT']\n",
    "bureau['bureau_AMT_CREDIT_SUM - AMT_CREDIT_SUM_DEBT'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "bureau['bureau_AMT_CREDIT_SUM - AMT_CREDIT_SUM_LIMIT'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_LIMIT']\n",
    "bureau['bureau_AMT_CREDIT_SUM - AMT_CREDIT_SUM_OVERDUE'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_OVERDUE']\n",
    "\n",
    "bureau['bureau_DAYS_CREDIT - CREDIT_DAY_OVERDUE'] = bureau['DAYS_CREDIT'] - bureau['CREDIT_DAY_OVERDUE']\n",
    "bureau['bureau_DAYS_CREDIT - DAYS_CREDIT_ENDDATE'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n",
    "bureau['bureau_DAYS_CREDIT - DAYS_ENDDATE_FACT'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n",
    "bureau['bureau_DAYS_CREDIT_ENDDATE - DAYS_ENDDATE_FACT'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "bureau['bureau_DAYS_CREDIT_UPDATE - DAYS_CREDIT_ENDDATE'] = bureau['DAYS_CREDIT_UPDATE'] - bureau['DAYS_CREDIT_ENDDATE']\n",
    "\n",
    "# Conduct Aggregation\n",
    "bureau_by_client = aggregate_client(bureau, group_vars =['SK_ID_BUREAU', 'SK_ID_CURR'], df_names = ['bureau', 'client'])\n",
    "\n",
    "list(bureau_by_client.columns)\n",
    "\n",
    "train=train.merge(bureau_by_client, on = 'SK_ID_CURR', how = 'left' )\n",
    "test=test.merge(bureau_by_client, on = 'SK_ID_CURR', how = 'left' )\n",
    "\n",
    "gc.enable()\n",
    "del bureau , bureau_by_client \n",
    "gc.collect()\n",
    "train = remove_missing_columns(train)\n",
    "train.info()\n",
    "train.to_csv('train_after_stage5_1.csv', index = False)\n",
    "test.to_csv('test_after_stage5_1.csv', index = False)\n",
    "#test_labels = test['TARGET']\n",
    "#test_labels.to_csv('test_labels.csv', index = False)\n",
    "#test1 = test.drop(columns =['TARGET'])\n",
    "########### 2. Bureau_balance Data Set #########################################################################################################################################\n",
    "# bureau: information about client's previous loans with other financial institutions reported to Home Credit. Each previous loan has its own row.\n",
    "bureau_balance = pd.read_csv('bureau_balance.csv')\n",
    "bureau_balance.head()\n",
    "bureau = pd.read_csv('bureau.csv')[['SK_ID_BUREAU', 'SK_ID_CURR']]\n",
    "bureau_balance = bureau_balance.merge(bureau, on ='SK_ID_BUREAU', how = 'left')\n",
    "\n",
    "bureau_balance = convert_types(bureau_balance, print_info = True)\n",
    "bureau_balance.info()\n",
    "\n",
    "bureau_balance_by_client = aggregate_client(bureau_balance, group_vars =['SK_ID_BUREAU', 'SK_ID_CURR'], df_names = ['bureau_balance', 'client'])\n",
    "\n",
    "bureau_balance_by_client.head()\n",
    "\n",
    "train=train.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "test=test.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "gc.enable()\n",
    "del bureau_balance_by_client, bureau_balance, bureau\n",
    "gc.collect()\n",
    "train, test = remove_missing_columns(train, test)\n",
    "train.info()\n",
    "\n",
    "train.to_csv('train_after_stage2_0.csv', index = False) # 970 features\n",
    "test.to_csv('test_after_stage2_0.csv', index = False)\n",
    "\n",
    "\n",
    "########### 3.previous_application Data Set #######################################################################################################################################################################################################################\n",
    "\n",
    "previous=pd.read_csv('previous_application.csv')\n",
    "previous = convert_types(previous, print_info=True)\n",
    "previous.head()\n",
    "\n",
    "previous['prev AMT_APPLICATION / AMT_CREDIT'] = previous['AMT_APPLICATION'] / previous['AMT_CREDIT']\n",
    "previous['prev AMT_APPLICATION - AMT_CREDIT'] = previous['AMT_APPLICATION'] - previous['AMT_CREDIT']\n",
    "previous['prev AMT_APPLICATION - AMT_GOODS_PRICE'] = previous['AMT_APPLICATION'] - previous['AMT_GOODS_PRICE']\n",
    "previous['prev AMT_GOODS_PRICE - AMT_CREDIT'] = previous['AMT_GOODS_PRICE'] - previous['AMT_CREDIT']\n",
    "previous['prev DAYS_FIRST_DRAWING - DAYS_FIRST_DUE'] = previous['DAYS_FIRST_DRAWING'] - previous['DAYS_FIRST_DUE']\n",
    "previous['prev DAYS_TERMINATION less -500'] = (previous['DAYS_TERMINATION'] < -500).astype(int)\n",
    "\n",
    "previous_agg = agg_numeric(previous, 'SK_ID_CURR', 'previous')\n",
    "previous_agg.shape # 37 columns -> 70 columns\n",
    "\n",
    "\n",
    "previous_counts = agg_categorical(previous, 'SK_ID_CURR', 'previous')\n",
    "previous_counts.shape # 37 columns -> 285 columns\n",
    "list(previous_counts.columns)\n",
    "\n",
    "# train = pd.read_csv('train_after_stage1.csv')\n",
    "train = convert_types(train)\n",
    "\n",
    "# Merge new features into train and test\n",
    "#train = train.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\n",
    "train = train.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "test = test.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Remove variables to free memory\n",
    "gc.enable()\n",
    "del previous, previous_agg, previous_counts\n",
    "gc.collect()\n",
    "\n",
    "train, test = remove_missing_columns(train, test)\n",
    "\n",
    "\n",
    "########### 4_ Monthly Cash Data Set #######################################################################################################################################################################################################################\n",
    "\n",
    "cash = pd.read_csv('POS_CASH_balance.csv')\n",
    "cash = convert_types(cash, print_info = True)\n",
    "cash.head()\n",
    "cash.info()\n",
    "\n",
    "# Replace some outliers\n",
    "cash.loc[cash['CNT_INSTALMENT_FUTURE'] > 60, 'CNT_INSTALMENT_FUTURE'] = np.nan\n",
    "    \n",
    "# Some new features\n",
    "cash['pos CNT_INSTALMENT more CNT_INSTALMENT_FUTURE'] = (cash['CNT_INSTALMENT'] > cash['CNT_INSTALMENT_FUTURE']).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cash_by_client = aggregate_client(cash, group_vars =['SK_ID_PREV', 'SK_ID_CURR'], df_names =['cash', 'client'])\n",
    "cash_by_client.info()\n",
    "cash_by_client.head()\n",
    "\n",
    "print('Cash by client Shape: ', cash_by_client.shape)\n",
    "train = train.merge(cash_by_client, on ='SK_ID_CURR', how ='left')\n",
    "\n",
    "gc.enable()\n",
    "del cash, cash_by_client\n",
    "gc.collect()\n",
    "\n",
    "train, test= remove_missing_columns(train, test)\n",
    "\n",
    "########### 5_ Monthly Credit Data Set #######################################################################################################################################################################################################################\n",
    "\n",
    "\n",
    "credit = pd.read_csv('credit_card_balance.csv')\n",
    "credit = convert_types(credit, print_info = True)\n",
    "credit.info()\n",
    "credit.head()\n",
    "\n",
    " # Replace some outliers\n",
    "credit.loc[credit['AMT_PAYMENT_CURRENT'] > 4000000, 'AMT_PAYMENT_CURRENT'] = np.nan\n",
    "credit.loc[credit['AMT_CREDIT_LIMIT_ACTUAL'] > 1000000, 'AMT_CREDIT_LIMIT_ACTUAL'] = np.nan\n",
    "\n",
    "# Some new features\n",
    "credit['credit_card_ missing'] = credit.isnull().sum(axis = 1).values\n",
    "credit['credit_card_SK_DPD - MONTHS_BALANCE'] = credit['SK_DPD'] - credit['MONTHS_BALANCE']\n",
    "credit['credit_card_SK_DPD_DEF - MONTHS_BALANCE'] = credit['SK_DPD_DEF'] - credit['MONTHS_BALANCE']\n",
    "credit['credit_card_SK_DPD - SK_DPD_DEF'] = credit['SK_DPD'] - credit['SK_DPD_DEF']\n",
    "    \n",
    "credit['credit_card_AMT_TOTAL_RECEIVABLE - AMT_RECIVABLE'] = credit['AMT_TOTAL_RECEIVABLE'] - credit['AMT_RECIVABLE']\n",
    "credit['credit_card_AMT_TOTAL_RECEIVABLE - AMT_RECEIVABLE_PRINCIPAL'] = credit['AMT_TOTAL_RECEIVABLE'] - credit['AMT_RECEIVABLE_PRINCIPAL']\n",
    "credit['credit_card_AMT_RECIVABLE - AMT_RECEIVABLE_PRINCIPAL'] = credit['AMT_RECIVABLE'] - credit['AMT_RECEIVABLE_PRINCIPAL']\n",
    "\n",
    "credit['credit_card_AMT_BALANCE - AMT_RECIVABLE'] = credit['AMT_BALANCE'] - credit['AMT_RECIVABLE']\n",
    "credit['credit_card_AMT_BALANCE - AMT_RECEIVABLE_PRINCIPAL'] = credit['AMT_BALANCE'] - credit['AMT_RECEIVABLE_PRINCIPAL']\n",
    "credit['credit_card_AMT_BALANCE - AMT_TOTAL_RECEIVABLE'] = credit['AMT_BALANCE'] - credit['AMT_TOTAL_RECEIVABLE']\n",
    "credit['credit_card_AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_ATM_CURRENT'] = credit['AMT_DRAWINGS_CURRENT'] - credit['AMT_DRAWINGS_ATM_CURRENT']\n",
    "credit['credit_card_AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_OTHER_CURRENT'] = credit['AMT_DRAWINGS_CURRENT'] - credit['AMT_DRAWINGS_OTHER_CURRENT']\n",
    "credit['credit_card_AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_POS_CURRENT'] = credit['AMT_DRAWINGS_CURRENT'] - credit['AMT_DRAWINGS_POS_CURRENT']\n",
    "\n",
    "credit_by_client = aggregate_client(credit, group_vars=['SK_ID_PREV', 'SK_ID_CURR'], df_names=['credit','client'])\n",
    "credit_by_client.head()\n",
    "\n",
    "train = train.merge(credit_by_client, on = 'SK_ID_CURR', how ='left')\n",
    "test = test.merge(credit_by_client, on = 'SK_ID_CURR', how ='left')\n",
    "\n",
    "gc.enable()\n",
    "del credit, credit_by_client\n",
    "gc.collect()\n",
    "train, test = remove_missing_columns(train, test)\n",
    "\n",
    "train.to_csv('train_after_stage2_1.csv', index = False) # 2600 features\n",
    "test.to_csv('train_after_stage2_1.csv', index = False)\n",
    "\n",
    "########### 6_ Installment Payments Data Set ##############################################################################\n",
    "\n",
    "installments =pd.read_csv('installments_payments.csv')\n",
    "installments = convert_types(installments, print_info = True)\n",
    "installments.info()\n",
    "installments.head()\n",
    "# Replace some outliers\n",
    "# Replace some outliers\n",
    "installments.loc[installments['NUM_INSTALMENT_VERSION'] > 70, 'NUM_INSTALMENT_VERSION'] = np.nan\n",
    "installments.loc[installments['DAYS_ENTRY_PAYMENT'] < -4000, 'DAYS_ENTRY_PAYMENT'] = np.nan\n",
    "\n",
    "\n",
    "# Percentage and difference paid in each installment (amount paid and installment value)\n",
    "installments['ins_PAYMENT_PERC'] = installments['AMT_PAYMENT'] / installments['AMT_INSTALMENT']\n",
    "installments['ins_PAYMENT_DIFF'] = installments['AMT_INSTALMENT'] - installments['AMT_PAYMENT']\n",
    "# Days past due and days before due (no negative values)\n",
    "installments['ins_DPD'] = installments['DAYS_ENTRY_PAYMENT'] - installments['DAYS_INSTALMENT']\n",
    "installments['ins_DBD'] = installments['DAYS_INSTALMENT'] - installments['DAYS_ENTRY_PAYMENT']\n",
    "installments['ins_DPD'] = installments['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "installments['ins_DBD'] = installments['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "# Others\n",
    "installments['ins_DAYS_ENTRY_PAYMENT - DAYS_INSTALMENT'] = installments['DAYS_ENTRY_PAYMENT'] - installments['DAYS_INSTALMENT']\n",
    "installments['ins_NUM_INSTALMENT_NUMBER_100'] = (installments['NUM_INSTALMENT_NUMBER'] == 100).astype(int)\n",
    "installments['ins_DAYS_INSTALMENT more NUM_INSTALMENT_NUMBER'] = (installments['DAYS_INSTALMENT'] > installments['NUM_INSTALMENT_NUMBER'] * 50 / 3 - 11500 / 3).astype(int)\n",
    "\n",
    "\n",
    "installments_by_client = aggregate_client(installments, group_vars =['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\n",
    "\n",
    "installments_by_client.head()\n",
    "\n",
    "train=train.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left' )\n",
    "test=test.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left' )\n",
    "\n",
    "gc.enable()\n",
    "del installments, installments_by_client\n",
    "gc.collect()\n",
    "train, test = remove_missing_columns(train, test)\n",
    "train.info()\n",
    "\n",
    "print(f'Final training size: {return_size(train)}')\n",
    "\n",
    "\n",
    "train.to_csv('train_after_stage3_1.csv', index = False) # 3200 features\n",
    "test.to_csv('test_after_stage3_1.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PATR 2: Import models that will be used ##############################################################################\n",
    "############## Model function for testing  ('logistic_model', 'random_forest_model', 'xg_boost_model', 'lgb_model') using Cross Validation\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def model(features, test_features, encoding = 'ohe', n_folds = 5, im ='median', used_model = 'lgb_model'):\n",
    "   \n",
    "    \"\"\"Train and test a model ('logistic_model', 'random_forest_model', 'xg_boost_model', 'lgb_model') using cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "        n_folds (int, default = 5):\n",
    "            number of folds to use for cross validation\n",
    "        used_model: \n",
    "            choose one of 4 following models ('logistic_model', 'random_forest_model', 'xg_boost_model', 'lgb_model')\n",
    "            \n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "    \"\"\"\n",
    "    # Extract the ids:  features = train , test_features= test\n",
    "    features, test_features = remove_missing_columns(features, test_features, threshold = 99.9)\n",
    "    train_ids = features ['SK_ID_CURR'] \n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    train.shape\n",
    "    test.shape\n",
    "    features.shape\n",
    "    test_features.shape\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    # One Hot Encoding\n",
    "    \n",
    "    if encoding =='ohe':\n",
    "        features =pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "        # Align the data by columns:\n",
    "        features, test_features = features.align(test_features, join='inner', axis = 1)\n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "     \n",
    "    # Integer Label Encoding\n",
    "    elif encoding == 'le':\n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        # Iterate through each column:\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to intergers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_featutes).astype(str).reshape((-1,)))\n",
    "                \n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "                \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    ##### Median imputation of missing values for 'logistic_model'|'random_forest_model'\n",
    "    if used_model == 'logistic_model'or'random_forest_model':\n",
    "        \n",
    "        if im=='median':\n",
    "        \n",
    "            imputer = Imputer(strategy ='median') # can replaced by 'most_frequent' or 'constant'\n",
    "\n",
    "            imputer.fit(features)\n",
    "            features = imputer.transform(features)\n",
    "            test_features = imputer.transform(test_features)\n",
    "        elif im =='zero':\n",
    "            features.fillna(0, inplace=True)\n",
    "            test_features.fillna(0, inplace=True)\n",
    "        elif im =='number':\n",
    "            features.fillna(30000, inplace=True)\n",
    "            test_features.fillna(30000, inplace=True)\n",
    "        else:\n",
    "            print(\"NA must be filled by 'median' or '0' or 'a number'\")\n",
    "        \n",
    "\n",
    "    ##### Scale to 0-1 for logistic regression\n",
    "    if used_model == 'logistic_model':\n",
    "        scaler = MinMaxScaler(feature_range =(0,1))\n",
    "        scaler.fit(features)\n",
    "        features = scaler.transform(features)\n",
    "        test_features = scaler.transform(test_features)\n",
    "   \n",
    "    \n",
    "    # Convert to np arrays\n",
    "    if used_model == 'xg_boost_model'or'lgb_model':\n",
    "        features = np.array(features)\n",
    "        test_features = np.array(test_features)\n",
    "      \n",
    "       \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    # Empty array for out of fold validation prediction\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    train_prediction = np.zeros(features.shape[0])\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    if used_model == 'logistic_model':\n",
    "        for train_indices, valid_indices in k_fold.split(features):\n",
    "            print(train_indices, valid_indices)\n",
    "            train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        \n",
    "            valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "            # Create the model\n",
    "        \n",
    "            model = LogisticRegression(C=0.0001)\n",
    "            # Train the model\n",
    "            model.fit(train_features, train_labels)\n",
    "                    \n",
    "            # Make predictions\n",
    "            train_prediction = model.predict_proba(train_features)[:,1]\n",
    "            train_auc = roc_auc_score(train_labels, train_prediction)\n",
    "            train_scores.append(train_auc)\n",
    "            valid_prediction = model.predict_proba(valid_features)[:,1]\n",
    "            valid_auc = roc_auc_score(valid_labels, valid_prediction)\n",
    "            valid_scores.append(valid_auc)\n",
    "            test_predictions += model.predict_proba(test_features)[:,1]/k_fold.n_splits\n",
    "        \n",
    "            # Record the out of fold predictions\n",
    "            out_of_fold[valid_indices] = model.predict_proba(valid_features)[:,1]\n",
    "        \n",
    "            # Clean up memory\n",
    "            gc.enable()\n",
    "            del model, train_features, valid_features\n",
    "            gc.collect()\n",
    "        \n",
    "        # Overall validation score\n",
    "        valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "        # Add the overall scores to the metrics\n",
    "        valid_scores.append(valid_auc)\n",
    "        train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "        # creating dataframe of validation scores\n",
    "        fold_names = list(range(n_folds))\n",
    "        fold_names.append('overall')\n",
    "    \n",
    "        # Dataframe of validation scores\n",
    "        metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores})\n",
    "        return metrics, test_predictions\n",
    "    \n",
    "    elif used_model == 'random_forest_model':\n",
    "        for train_indices, valid_indices in k_fold.split(features):\n",
    "            train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "            valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "            # Create the model\n",
    "        \n",
    "            model = RandomForestClassifier(n_estimators = 1000, random_state = 50, verbose = 1, n_jobs = -1, max_depth= 10)\n",
    "            # Train the model\n",
    "            model.fit(train_features, train_labels)\n",
    "            #model.fit(features, labels)\n",
    "        \n",
    "            # Record the feature importances\n",
    "            feature_importance_values += model.feature_importances_/k_fold.n_splits \n",
    "                \n",
    "            # Make predictions\n",
    "            train_prediction = model.predict_proba(train_features)[:,1]\n",
    "            train_auc = roc_auc_score(train_labels, train_prediction)\n",
    "            train_scores.append(train_auc)\n",
    "            valid_prediction = model.predict_proba(valid_features)[:,1]\n",
    "            valid_auc = roc_auc_score(valid_labels, valid_prediction)\n",
    "            valid_scores.append(valid_auc)\n",
    "            test_predictions += model.predict_proba(test_features)[:,1]/k_fold.n_splits\n",
    "        \n",
    "            # Record the out of fold predictions\n",
    "            out_of_fold[valid_indices] = model.predict_proba(valid_features)[:,1]\n",
    "        \n",
    "            # Clean up memory\n",
    "            gc.enable()\n",
    "            del model, train_features, valid_features\n",
    "            gc.collect()\n",
    "        \n",
    "        # Overall validation score\n",
    "        valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "        # Add the overall scores to the metrics\n",
    "        valid_scores.append(valid_auc)\n",
    "        train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "        # creating dataframe of validation scores\n",
    "        fold_names = list(range(n_folds))\n",
    "        fold_names.append('overall')\n",
    "    \n",
    "    \n",
    "        # Dataframe of validation scores\n",
    "        metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores})\n",
    "    \n",
    "        # Make the feature importance dataframe\n",
    "        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values })\n",
    "        feature_importances = feature_importances.sort_values('importance', ascending = False)\n",
    "        return metrics, feature_importances, test_predictions\n",
    "    elif used_model == 'xg_boost_model':\n",
    "        for train_indices, valid_indices in k_fold.split(features):\n",
    "            train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "            valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "            # Create the model\n",
    "        \n",
    "            params = {'objective': 'binary:logistic',\n",
    "                      'max_depth': 5,\n",
    "                      'learning_rate': 0.05,# 0.005\n",
    "                      'silent': False,\n",
    "                      'n_estimators': 5000,\n",
    "                      'n_jobs=':-1\n",
    "                  }\n",
    "            \n",
    "            #params = {'objective': 'binary:logistic',\n",
    "            #          'max_depth': 5,\n",
    "            #          'learning_rate': 0.01,# 0.005\n",
    "            #          'silent': False,\n",
    "            #          'n_estimators': 5000,\n",
    "            #          \"gamma\": 0.0, \n",
    "            #          \"min_child_weight\": 10, # default: 1\n",
    "            #          \"subsample\": 0.7, \n",
    "            #          \"colsample_bytree\": 0.7,  # default:  1.0\n",
    "            #          \"colsample_bylevel\": 0.5, # default: 1.0\n",
    "            #          \"reg_alpha\": 0.0, \n",
    "            #          \"reg_lambda\": 1.0, \n",
    "            #          \"scale_pos_weight\": 1.0, \n",
    "            #          \"random_state\": 0,\n",
    "            ##          #\n",
    "            #          \"silent\": False, \n",
    "            #          \"n_jobs\": 16, \n",
    "            #          #\n",
    "            #          \"tree_method\": \"gpu_hist\", # default: auto\n",
    "            #          \"grow_policy\": \"lossguide\", # default depthwise\n",
    "            #          \"max_leaves\": 0, # default: 0(unlimited)\n",
    "            #          \"max_bin\": 256  # default: 256\n",
    "            #          }\n",
    "        \n",
    "            model = XGBClassifier(**params)\n",
    "            # Train the model\n",
    "        \n",
    "            model.fit(train_features, train_labels,eval_set = [(train_features, train_labels), (valid_features, valid_labels)],\n",
    "                                                               eval_metric = 'auc', early_stopping_rounds = 100, verbose=True)\n",
    "            # record the best iteration\n",
    "        \n",
    "            best_iteration = model.best_iteration\n",
    "                       \n",
    "            # Record the feature importances\n",
    "            feature_importance_values += model.feature_importances_/k_fold.n_splits \n",
    "                \n",
    "            # Make predictions\n",
    "            train_prediction = model.predict_proba(train_features)[:,1]\n",
    "            train_auc = roc_auc_score(train_labels, train_prediction)\n",
    "            train_scores.append(train_auc)\n",
    "            valid_prediction = model.predict_proba(valid_features)[:,1]\n",
    "            valid_auc = roc_auc_score(valid_labels, valid_prediction)\n",
    "            valid_scores.append(valid_auc)\n",
    "            test_predictions += model.predict_proba(test_features, ntree_limit = best_iteration)[:,1]/k_fold.n_splits\n",
    "        \n",
    "            # Record the out of fold predictions\n",
    "            out_of_fold[valid_indices] = model.predict_proba(valid_features, ntree_limit = best_iteration)[:,1]\n",
    "        \n",
    "            # Clean up memory\n",
    "            gc.enable()\n",
    "            del model, train_features, valid_features\n",
    "            gc.collect()\n",
    "        \n",
    "        # Overall validation score\n",
    "        valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "        # Add the overall scores to the metrics\n",
    "        valid_scores.append(valid_auc)\n",
    "        train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "        # creating dataframe of validation scores\n",
    "        fold_names = list(range(n_folds))\n",
    "        fold_names.append('overall')\n",
    "    \n",
    "    \n",
    "        # Dataframe of validation scores\n",
    "        metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores})\n",
    "    \n",
    "        # Make the feature importance dataframe\n",
    "        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values })\n",
    "        feature_importances = feature_importances.sort_values('importance', ascending = False)\n",
    "        return metrics, feature_importances, test_predictions\n",
    "    else:\n",
    "        for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "            # Training data for the fold\n",
    "            train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "            # Validation data for the fold\n",
    "            valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "            # Create the model\n",
    "            model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "            # Train the model\n",
    "            model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "            # Record the best iteration\n",
    "            best_iteration = model.best_iteration_\n",
    "        \n",
    "            # Record the feature importances\n",
    "            feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "            # Make predictions\n",
    "            test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "            # Record the out of fold predictions\n",
    "            out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "            # Record the best score\n",
    "            valid_score = model.best_score_['valid']['auc']\n",
    "            train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "            valid_scores.append(valid_score)\n",
    "            train_scores.append(train_score)\n",
    "        \n",
    "            # Clean up memory\n",
    "            gc.enable()\n",
    "            del model, train_features, valid_features\n",
    "            gc.collect()\n",
    "        \n",
    "        # Make the submission dataframe\n",
    "        submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    \n",
    "        # Make the feature importance dataframe\n",
    "        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "        # Overall validation score\n",
    "        valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "        # test_auc = roc_auc_score(test_labels, test_predictions)\n",
    "    \n",
    "        # Add the overall scores to the metrics\n",
    "        valid_scores.append(valid_auc)\n",
    "        train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "        # Needed for creating dataframe of validation scores\n",
    "        fold_names = list(range(n_folds))\n",
    "        fold_names.append('overall')\n",
    "    \n",
    "        # Dataframe of validation scores\n",
    "        metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "        return metrics, feature_importances, test_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PART 3: Test  Models with 'train_after_stage2_1.csv' data ############\n",
    "#########Test  Models with 'train_after_stage2_1.csv' data ####################\n",
    "# os.chdir('C:\\\\Users\\\\Phong\\\\')\n",
    "app_train = pd.read_csv('train_after_stage2_1.csv') # 1875 features\n",
    "# app_train = convert_types(app_train, print_info = True)\n",
    "print('Finish reading data, start spliting data into train, test')\n",
    "train, test = split_train_test(app_train, 0.2)\n",
    "train.to_csv('train_after_stage2_1_1.csv', index = False)\n",
    "test.to_csv('test_after_stage2_1_1.csv', index = False)\n",
    "gc.enable()\n",
    "del app_train\n",
    "gc.collect()\n",
    "print('Finish record the data, read it again')\n",
    "print('Eliminate infinity values and remove columns with missing values > 99%')\n",
    "train = pd.read_csv('train_after_stage2_1_1.csv')\n",
    "test = pd.read_csv('test_after_stage2_1_1.csv').drop(columns =['TARGET'])\n",
    "test_labels =  pd.read_csv('test_after_stage2_1_1.csv')['TARGET']\n",
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)\n",
    "train, test = remove_missing_columns(train, test, threshold = 99)\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "\n",
    "    \n",
    "# Logistic Model\n",
    "print('Start running Logistic Model for train_after_stage2_1.csv data ')\n",
    "auc_lg2, prediction_lg2= model(train, test, used_model ='logistic_model')  \n",
    "test_auc_lg2 = roc_auc_score(test_labels, prediction_lg2)\n",
    "logic = ['No']*5\n",
    "logic.append(test_auc_lg2)\n",
    "auc_lg2['logic'] = logic\n",
    "auc_lg2\n",
    "# Random Forest Model\n",
    "print('Start running Random Forest Model for train_after_stage2_1.csv data ')\n",
    "auc_rf2, feature_importances_rf2, prediction_rf2= model(train, test, used_model ='random_forest_model') \n",
    "test_auc_rf2 = roc_auc_score(test_labels, prediction_rf2)\n",
    "logic = ['No']*5\n",
    "logic.append(test_auc_rf2)\n",
    "auc_rf2['random_forest'] = logic\n",
    "auc_rf2\n",
    "# Light Gradient Boosting Model\n",
    "print('Start running Light Gradient Boosting Model for train_after_stage2_1.csv data ')\n",
    "auc_lgb2, feature_importances_lgb2, prediction_lgb2 = model(train, test, used_model ='lgb_model') # \n",
    "test_auc_lgb2 = roc_auc_score(test_labels, prediction_lgb2)\n",
    "logic = ['No']*5\n",
    "logic.append(test_auc_lgb2)\n",
    "auc_lgb2['light gradient boosting'] = logic\n",
    "auc_lgb2\n",
    "\n",
    "# XG boosting Model\n",
    "print('Start running XG boosting Model for train_after_stage2_1.csv data ')\n",
    "auc_xg2, feature_importances_xg2, prediction_xg2 =  model(train, test, used_model ='xg_boost_model')\n",
    "test_auc_xg2 = roc_auc_score(test_labels, prediction_xg2)\n",
    "logic = ['No']*5\n",
    "logic.append(test_auc_xg2)\n",
    "auc_xg2['xg_boosting'] = logic\n",
    "auc_xg2\n",
    "print('Summary results for train_after_stage2_1.csv data ')\n",
    "testing_summary_stage2 = pd.concat([auc_lg2,auc_rf2, auc_xg2, auc_lgb2 ], axis=1)\n",
    "testing_summary_stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PART 4: Test  Models with 'train_after_stage3_1.csv' data  ############\n",
    "app_train = pd.read_csv('train_after_stage3_1.csv') # 'application_train.csv'\n",
    "#app_train = convert_types(app_train, print_info = True)\n",
    "print('Finish reading data, start spliting data into train, test')\n",
    "\n",
    "train, test = split_train_test(app_train, 0.2)\n",
    "train.to_csv('train_after_stage3_1_1.csv', index = False)\n",
    "test.to_csv('test_after_stage3_1_1.csv', index = False)\n",
    "gc.enable()\n",
    "del app_train\n",
    "gc.collect()\n",
    "print('Finish record the data, read it again')\n",
    "print('Eliminate infinity values and remove columns with missing values > 99%')\n",
    "\n",
    "train = pd.read_csv('train_after_stage3_1_1.csv')\n",
    "test = pd.read_csv('train_after_stage3_1_1.csv').drop(columns =['TARGET'])\n",
    "test_labels =  pd.read_csv('train_after_stage3_1_1.csv')['TARGET']\n",
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)\n",
    "train, test = remove_missing_columns(train, test, threshold = 99)\n",
    "\n",
    "\n",
    "# Logistic Model\n",
    "print('Start running Logistic Model for train_after_stage3_1.csv data ')\n",
    "auc_lg3, prediction_lg3=model(train, test, used_model ='logistic_model') \n",
    "test_auc_lg3 = roc_auc_score(test_labels, prediction_lg3)\n",
    "logic = ['No']*5\n",
    "logic.append(test_auc_lg3)\n",
    "auc_lg3['logic'] = logic\n",
    "auc_lg3\n",
    "# Random Forest Model\n",
    "print('Start running Random Forest Model for train_after_stage3_1.csv data ')\n",
    "auc_rf3, feature_importances_rf3, prediction_rf3 =  model(train, test, used_model ='random_forest_model') \n",
    "test_auc_rf3 = roc_auc_score(test_labels, prediction_rf3)\n",
    "logic = ['No']*5\n",
    "logic.append(test_auc_rf3)\n",
    "auc_rf3['random_forest'] = logic\n",
    "auc_rf3\n",
    "# Light Gradient Boosting Model\n",
    "print('Start running Light Gradient Boosting Model for train_after_stage3_1.csv data ')\n",
    "auc_lgb3, feature_importances_lgb3, prediction_lgb3 = model(train, test, used_model ='lgb_model')# \n",
    "test_auc_lgb3 = roc_auc_score(test_labels, prediction_lgb3)\n",
    "logic = ['No']*5\n",
    "logic.append(test_auc_lgb3)\n",
    "auc_lgb3['light gradient boosting'] = logic\n",
    "auc_lgb3\n",
    "# XG boosting Model\n",
    "print('Start running XG boosting Model for train_after_stage3_1.csv data ') \n",
    "auc_xg3, feature_importances_xg3, prediction_xg3 =  model(train, test, used_model ='xg_boost_model')\n",
    "test_auc_xg3 = roc_auc_score(test_labels, prediction_xg3)\n",
    "\n",
    "logic = ['No']*5\n",
    "logic.append(test_auc_xg3)\n",
    "auc_xg3['xg_boosting'] = logic\n",
    "auc_xg3\n",
    "\n",
    "print('Summary results for train_after_stage3_1.csv data ')\n",
    "testing_summary_stage3 = pd.concat([auc_lg3,auc_rf3, auc_xg3,auc_lgb3 ], axis=1) \n",
    "testing_summary_stage3\n",
    "\n",
    "print('Summary results for train_after_stage2_1.csv and train_after_stage3_1.csv data ')\n",
    "testing_summary = pd.concat([testing_summary_stage2, testing_summary_stage3], axis=0)\n",
    "\n",
    "testing_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
